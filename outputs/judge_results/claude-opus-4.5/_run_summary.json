{
  "judge_model": "anthropic/claude-opus-4.5",
  "timestamp": "2026-02-07T14:07:28.405602",
  "cases_evaluated": 20,
  "prompt_template": "You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.\n\nCompare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).\n\n## REFERENCE SUMMARY (Ground Truth):\n{ground_truth}\n\n## GENERATED SUMMARY (To Evaluate):\n{llm_summary}\n\n## AUDIT INSTRUCTIONS:\n\n1. **Step 1: Accuracy Check (Hallucinations)**:\n   - Does the summary contain facts not in the reference?\n   - **Severity Assessment**:\n     - *Trivial* (Detailed date wrong, minor phrasing): **Minor Issue (-1)**\n     - *Substantial* (Invented detail, wrong party name): **Major Issue (-2)**\n     - *Critical* (Changes the legal outcome/holding): **Critical Issue (-3)**\n\n2. **Step 2: Legal Precision Check (Holding/Reasoning)**:\n   - Did it miss the main holding? **Critical Issue (-3)**\n   - Did it miss a key reasoning step? **Major Issue (-2)**\n\n3. **Step 3: Completeness Check (Opinions)**:\n   - Missed the **Dissent**? **Major Omission (-2)**\n   - Missed a **Concurrence**? **Minor Omission (-1)**\n\n## SCORING RUBRIC (1-5 Integer Scale):\nScore based on the *worst* error found. Do not sum penalties below 1.\n\n- **5 (Perfect)**: Flawless. No hallucinations, covers holding, reasoning, concurrences, and dissents.\n- **4 (Minor Flaw)**: Generally accurate but missed a **Concurrence** OR has a **Trivial** factual error (e.g., typos, minor date slip).\n- **3 (Major Flaw)**: Missed the **Dissent** OR has a **Substantial** hallucination (clearly wrong but doesn't change outcome).\n- **2 (Critical Failure)**: Missed the **Main Holding** OR has a **Critical** hallucination (reversed law/facts).\n- **1 (Catastrophic)**: Completely wrong case, incoherent, or dangerously misleading.\n\n## RESPOND IN THIS EXACT JSON FORMAT:\n```json\n{{\n    \"factual_accuracy\": <1-5 integer>,\n    \"completeness\": <1-5 integer>,\n    \"factual_errors\": [\n        {{\n            \"error_quote\": \"<exact quote>\",\n            \"issue\": \"<why is this wrong?>\",\n            \"severity\": \"<Minor/Major/Critical>\",\n            \"correct_info\": \"<what reference says>\"\n        }}\n    ],\n    \"hedging_detected\": <true/false>,\n    \"hedging_examples\": [],\n    \"key_omissions\": [\"<missing item 1>\", \"<missing item 2>\"],\n    \"overall_assessment\": \"<Brief explanation of the score based on the rubric buckets>\"\n}}\n```\n\nReturn ONLY the JSON."
}