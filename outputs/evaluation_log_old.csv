Timestamp,Run_ID,Case,Summarizer_Model,Judge_Model,Composite_Score,NLI_Score,Judge_Score,Coverage_Score,Factual_Accuracy,Completeness,Summarizer_Prompt,Judge_Prompt
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,1 Ontario v. Quon,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6524,0.7143,0.5000,0.8095,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,1 Ontario v. Quon,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7710,0.9048,0.6000,0.8571,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,1 Ontario v. Quon,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6290,0.7619,0.4000,0.8095,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,10 California v. Texas,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5864,0.4545,0.5000,0.9091,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,10 California v. Texas,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6355,0.5455,0.6000,0.8182,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,10 California v. Texas,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.4964,0.4091,0.4000,0.7727,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,11 NCAA v. Alston,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6600,0.6000,0.5000,1.0000,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,11 NCAA v. Alston,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7467,0.7333,0.6000,1.0000,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,11 NCAA v. Alston,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6733,0.8000,0.4000,0.9333,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6600,0.8333,0.4000,0.8333,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8046,0.9583,0.6000,0.9167,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6017,0.6667,0.4000,0.8333,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5883,0.6957,0.4000,0.7391,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7770,0.9130,0.6000,0.8696,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5970,0.7826,0.4000,0.6522,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6406,0.9167,0.4000,0.6389,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7331,0.8333,0.6000,0.8056,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.4850,0.6111,0.4000,0.4444,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,15 Kansas v. Glover,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6023,0.5000,0.5000,0.9091,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,15 Kansas v. Glover,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6741,0.5909,0.6000,0.9091,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,15 Kansas v. Glover,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5945,0.3636,0.6000,0.9091,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5350,0.5000,0.4000,0.8000,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6017,0.3667,0.6000,0.9333,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.4183,0.9667,0.2000,0.0000,1,1,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,17 Gill v. Whitford,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5689,0.6071,0.4000,0.7857,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,17 Gill v. Whitford,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6989,0.7500,0.6000,0.7857,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,17 Gill v. Whitford,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5850,0.6786,0.4000,0.7500,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8547,0.7368,0.9000,0.9474,5,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6458,0.3684,0.7000,0.9474,2,5,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5342,0.3158,0.5000,0.8947,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,2 Allen v. Michigan,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7325,0.9500,0.5000,0.8000,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,2 Allen v. Michigan,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6475,0.6000,0.5000,0.9500,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,2 Allen v. Michigan,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.2550,0.5000,0.2000,0.0000,1,1,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6059,0.5294,0.5000,0.8824,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7635,0.8235,0.6000,0.9412,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6059,0.5294,0.5000,0.8824,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5917,0.5833,0.5000,0.7500,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7400,0.8333,0.6000,0.8333,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5250,0.3333,0.5000,0.8333,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5924,0.6471,0.4000,0.8235,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7088,0.8235,0.5000,0.8824,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.4071,0.1176,0.4000,0.8235,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5779,0.6071,0.4000,0.8214,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6929,0.8214,0.5000,0.8214,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6225,0.7857,0.4000,0.7500,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6673,0.7308,0.5000,0.8462,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6727,0.5769,0.6000,0.9231,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5596,0.4231,0.5000,0.8462,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,7 Torres v. Madrid,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5056,0.2778,0.5000,0.8333,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,7 Torres v. Madrid,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.5733,0.2778,0.6000,0.9444,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,7 Torres v. Madrid,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5789,0.3333,0.6000,0.8889,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.5838,0.6667,0.4000,0.7619,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7376,0.8095,0.6000,0.8571,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.5286,0.4286,0.5000,0.7143,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,9 Van Buren v. United States,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6060,0.5600,0.5000,0.8400,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,9 Van Buren v. United States,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.6340,0.4400,0.6000,0.9600,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:07:07.928318,2026-02-07T13:07:07.928318_1931784314944,9 Van Buren v. United States,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6780,0.8800,0.5000,0.6800,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T10:29:07.067676,backfill_2026-02-07T10:29:07.067676_1 Ontario v. Quon,1 Ontario v. Quon,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:29:39.254219,backfill_2026-02-07T10:29:39.254219_1 Ontario v. Quon,1 Ontario v. Quon,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:30:24.380761,backfill_2026-02-07T10:30:24.380761_1 Ontario v. Quon,1 Ontario v. Quon,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:30:49.603383,backfill_2026-02-07T10:30:49.603383_10 California v. Texas,10 California v. Texas,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:32:15.622232,backfill_2026-02-07T10:32:15.622232_10 California v. Texas,10 California v. Texas,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:32:48.489678,backfill_2026-02-07T10:32:48.489678_10 California v. Texas,10 California v. Texas,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:33:18.169428,backfill_2026-02-07T10:33:18.169428_11 NCAA v. Alston,11 NCAA v. Alston,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:34:04.782651,backfill_2026-02-07T10:34:04.782651_11 NCAA v. Alston,11 NCAA v. Alston,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:34:50.790398,backfill_2026-02-07T10:34:50.790398_11 NCAA v. Alston,11 NCAA v. Alston,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:35:23.118390,backfill_2026-02-07T10:35:23.118390_12 Financial Oversight Board v. Aurelius,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:36:51.285077,backfill_2026-02-07T10:36:51.285077_12 Financial Oversight Board v. Aurelius,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:37:16.051136,backfill_2026-02-07T10:37:16.051136_12 Financial Oversight Board v. Aurelius,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:37:46.498586,backfill_2026-02-07T10:37:46.498586_13 Espinoza v. Montana Dept of Revenue,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:38:13.942435,backfill_2026-02-07T10:38:13.942435_13 Espinoza v. Montana Dept of Revenue,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:38:45.938008,backfill_2026-02-07T10:38:45.938008_13 Espinoza v. Montana Dept of Revenue,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:39:10.739213,backfill_2026-02-07T10:39:10.739213_14 American Legion v. American Humanist Assn,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:41:12.303940,backfill_2026-02-07T10:41:12.303940_14 American Legion v. American Humanist Assn,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:42:06.542657,backfill_2026-02-07T10:42:06.542657_14 American Legion v. American Humanist Assn,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.4000,0.0000,2,2,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:42:26.647861,backfill_2026-02-07T10:42:26.647861_15 Kansas v. Glover,15 Kansas v. Glover,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:42:43.645030,backfill_2026-02-07T10:42:43.645030_15 Kansas v. Glover,15 Kansas v. Glover,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:43:49.956994,backfill_2026-02-07T10:43:49.956994_15 Kansas v. Glover,15 Kansas v. Glover,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:45:21.855011,backfill_2026-02-07T10:45:21.855011_16 Knick v. Township of Scott,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:47:19.498278,backfill_2026-02-07T10:47:19.498278_16 Knick v. Township of Scott,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,5,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:47:36.212672,backfill_2026-02-07T10:47:36.212672_16 Knick v. Township of Scott,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.2000,0.0000,1,1,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:49:37.983013,backfill_2026-02-07T10:49:37.983013_17 Gill v. Whitford,17 Gill v. Whitford,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:50:06.562164,backfill_2026-02-07T10:50:06.562164_17 Gill v. Whitford,17 Gill v. Whitford,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:52:05.725000,backfill_2026-02-07T10:52:05.725000_17 Gill v. Whitford,17 Gill v. Whitford,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:52:46.275993,backfill_2026-02-07T10:52:46.275993_19 Encino Motorcars v. Navarro,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:53:18.738858,backfill_2026-02-07T10:53:18.738858_19 Encino Motorcars v. Navarro,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:53:44.951417,backfill_2026-02-07T10:53:44.951417_19 Encino Motorcars v. Navarro,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:54:18.845846,backfill_2026-02-07T10:54:18.845846_2 Allen v. Michigan,2 Allen v. Michigan,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:55:43.922711,backfill_2026-02-07T10:55:43.922711_2 Allen v. Michigan,2 Allen v. Michigan,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:56:17.631549,backfill_2026-02-07T10:56:17.631549_2 Allen v. Michigan,2 Allen v. Michigan,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.2000,0.0000,1,1,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:58:04.129416,backfill_2026-02-07T10:58:04.129416_20 Epic Systems v. Lewis,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:58:54.510569,backfill_2026-02-07T10:58:54.510569_20 Epic Systems v. Lewis,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T10:59:45.828228,backfill_2026-02-07T10:59:45.828228_20 Epic Systems v. Lewis,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:00:26.772235,backfill_2026-02-07T11:00:26.772235_3 Brown v. Board of Education of Topeka,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:02:46.038467,backfill_2026-02-07T11:02:46.038467_3 Brown v. Board of Education of Topeka,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:03:11.128112,backfill_2026-02-07T11:03:11.128112_3 Brown v. Board of Education of Topeka,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:35:27.241979,"backfill_2026-02-07T11:35:27.241979_4 Muldrow v. City of St. Louis, Missouri","4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:37:35.140724,"backfill_2026-02-07T11:37:35.140724_4 Muldrow v. City of St. Louis, Missouri","4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:37:50.912339,"backfill_2026-02-07T11:37:50.912339_4 Muldrow v. City of St. Louis, Missouri","4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.7000,0.0000,4,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:39:54.026288,backfill_2026-02-07T11:39:54.026288_5 Citizens United v. Federal Election Commission,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:40:50.624020,backfill_2026-02-07T11:40:50.624020_5 Citizens United v. Federal Election Commission,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:41:22.595827,backfill_2026-02-07T11:41:22.595827_5 Citizens United v. Federal Election Commission,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.5000,0.0000,3,2,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:42:23.380246,backfill_2026-02-07T11:42:23.380246_6 Google LLC v. Oracle America Inc,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,1.0000,0.0000,5,5,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:45:07.289719,backfill_2026-02-07T11:45:07.289719_6 Google LLC v. Oracle America Inc,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:46:07.772561,backfill_2026-02-07T11:46:07.772561_6 Google LLC v. Oracle America Inc,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.5000,0.0000,2,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:47:51.903714,backfill_2026-02-07T11:47:51.903714_7 Torres v. Madrid,7 Torres v. Madrid,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.4000,0.0000,2,2,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:49:14.950331,backfill_2026-02-07T11:49:14.950331_7 Torres v. Madrid,7 Torres v. Madrid,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.9000,0.0000,5,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:49:52.326554,backfill_2026-02-07T11:49:52.326554_7 Torres v. Madrid,7 Torres v. Madrid,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.5000,0.0000,2,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:50:25.631078,backfill_2026-02-07T11:50:25.631078_8 Fulton v. City of Philadelphia,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:53:01.363850,backfill_2026-02-07T11:53:01.363850_8 Fulton v. City of Philadelphia,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:53:34.535077,backfill_2026-02-07T11:53:34.535077_8 Fulton v. City of Philadelphia,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.8000,0.0000,4,4,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:54:25.927548,backfill_2026-02-07T11:54:25.927548_9 Van Buren v. United States,9 Van Buren v. United States,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.0000,0.0000,0.5000,0.0000,2,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:55:17.795106,backfill_2026-02-07T11:55:17.795106_9 Van Buren v. United States,9 Van Buren v. United States,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.0000,0.0000,0.5000,0.0000,2,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T11:57:11.420963,backfill_2026-02-07T11:57:11.420963_9 Van Buren v. United States,9 Van Buren v. United States,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.0000,0.0000,0.6000,0.0000,3,3,SEE_SRC_SUMMARIZER_PY,"You are an expert legal evaluator assessing the quality of AI-generated Supreme Court case summaries.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth) and evaluate faithfulness.

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## EVALUATION CRITERIA:

1. **Factual Accuracy (1-5)**: Does the generated summary accurately represent the facts, parties, legal issues, and holdings? Are there any factual errors or misrepresentations?
   - 5: Perfectly accurate, no errors
   - 4: Minor inaccuracies that don't affect understanding
   - 3: Some errors but core facts correct
   - 2: Significant errors affecting understanding
   - 1: Major factual errors or misrepresentations

2. **Completeness (1-5)**: Does the generated summary cover all key points from the reference?
   - 5: Covers everything important
   - 4: Missing minor details only
   - 3: Missing some notable points
   - 2: Missing significant information
   - 1: Missing critical information

3. **Factual Errors/Inaccuracies**: Identify SPECIFIC factual errors in the generated summary. For each error:
   - Quote the problematic text from the generated summary
   - Explain what is wrong
   - Quote the correct information from the reference

4. **Hedging/Softening Detection**: Did the summary weaken, hedge, or soften any definitive claims from the reference? (e.g., ""the Court held"" -> ""the Court suggested"")

5. **Key Omissions**: What specific important information from the reference is missing in the generated summary?

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{
    ""factual_accuracy"": <1-5>,
    ""completeness"": <1-5>,
    ""factual_errors"": [
        {
            ""error_quote"": ""<exact quote from generated summary that is wrong>"",
            ""issue"": ""<explanation of what is incorrect>"",
            ""correct_info"": ""<what the reference actually says>""
        }
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [
        {
            ""summary_says"": ""<quote from generated summary>"",
            ""reference_says"": ""<quote from reference showing stronger language>"",
            ""issue"": ""<explanation of how it was softened>""
        }
    ],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<brief 2-3 sentence assessment of the summary quality>""
}
```

Return ONLY the JSON, no other text."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,1 Ontario v. Quon,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6124,0.7143,0.4000,0.8095,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,1 Ontario v. Quon,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7710,0.9048,0.6000,0.8571,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,1 Ontario v. Quon,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6290,0.7619,0.4000,0.8095,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,10 California v. Texas,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6664,0.4545,0.7000,0.9091,4,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,10 California v. Texas,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7555,0.5455,0.9000,0.8182,4,5,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,10 California v. Texas,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.5764,0.4091,0.6000,0.7727,4,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,11 NCAA v. Alston,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7400,0.6000,0.7000,1.0000,4,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,11 NCAA v. Alston,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7467,0.7333,0.6000,1.0000,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,11 NCAA v. Alston,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7933,0.8000,0.7000,0.9333,4,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7000,0.8333,0.5000,0.8333,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8046,0.9583,0.6000,0.9167,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6417,0.6667,0.5000,0.8333,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6683,0.6957,0.6000,0.7391,4,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7770,0.9130,0.6000,0.8696,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6770,0.7826,0.6000,0.6522,4,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6406,0.9167,0.4000,0.6389,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7331,0.8333,0.6000,0.8056,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.4850,0.6111,0.4000,0.4444,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,15 Kansas v. Glover,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6023,0.5000,0.5000,0.9091,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,15 Kansas v. Glover,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6741,0.5909,0.6000,0.9091,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,15 Kansas v. Glover,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.5945,0.3636,0.6000,0.9091,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.5750,0.5000,0.5000,0.8000,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6017,0.3667,0.6000,0.9333,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.4183,0.9667,0.2000,0.0000,1,1,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,17 Gill v. Whitford,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6489,0.6071,0.6000,0.7857,4,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,17 Gill v. Whitford,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6989,0.7500,0.6000,0.7857,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,17 Gill v. Whitford,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.5850,0.6786,0.4000,0.7500,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.8147,0.7368,0.8000,0.9474,4,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7258,0.3684,0.9000,0.9474,4,5,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6542,0.3158,0.8000,0.8947,4,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,2 Allen v. Michigan,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7325,0.9500,0.5000,0.8000,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,2 Allen v. Michigan,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6475,0.6000,0.5000,0.9500,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,2 Allen v. Michigan,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.2550,0.5000,0.2000,0.0000,1,1,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6059,0.5294,0.5000,0.8824,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7635,0.8235,0.6000,0.9412,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7259,0.5294,0.8000,0.8824,4,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.5917,0.5833,0.5000,0.7500,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8200,0.8333,0.8000,0.8333,4,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.5250,0.3333,0.5000,0.8333,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.5924,0.6471,0.4000,0.8235,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7488,0.8235,0.6000,0.8824,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.4071,0.1176,0.4000,0.8235,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6179,0.6071,0.5000,0.8214,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6929,0.8214,0.5000,0.8214,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6225,0.7857,0.4000,0.7500,2,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6673,0.7308,0.5000,0.8462,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6727,0.5769,0.6000,0.9231,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.5596,0.4231,0.5000,0.8462,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,7 Torres v. Madrid,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.5056,0.2778,0.5000,0.8333,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,7 Torres v. Madrid,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6133,0.2778,0.7000,0.9444,2,5,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,7 Torres v. Madrid,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.5789,0.3333,0.6000,0.8889,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6638,0.6667,0.6000,0.7619,4,2,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8576,0.8095,0.9000,0.8571,4,5,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6086,0.4286,0.7000,0.7143,4,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,9 Van Buren v. United States,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6060,0.5600,0.5000,0.8400,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,9 Van Buren v. United States,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.6340,0.4400,0.6000,0.9600,2,4,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T13:28:55.235781,2026-02-07T13:28:55.235781_2072428963136,9 Van Buren v. United States,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6780,0.8800,0.5000,0.6800,2,3,"You are a legal expert summarizing Supreme Court cases for educational purposes.

Summarize the following court opinion. Your summary MUST cover these three sections:

1. **Facts of the Case**: What happened? Who are the parties? What led to this case reaching the Supreme Court?

2. **Legal Question(s)**: What specific constitutional or legal question(s) did the Court address?

3. **Conclusion/Holding**: How did the Court rule? What was the reasoning? What is the precedent set?

IMPORTANT INSTRUCTIONS:
- Be factual and comprehensive
- Do NOT add information not present in the source document
- Do NOT speculate or make assumptions
- Use clear, professional legal language
- Include specific details, parties, and legal citations mentioned in the opinion

---

COURT OPINION:

{opinion_text}

---

Provide your summary below:","You are a highly critical, pedantic legal scholar auditor. Your job is to find ANY and ALL errors in AI-generated Supreme Court case summaries.

You are NOT here to be nice. You are here to detect failure.

Compare the GENERATED SUMMARY against the REFERENCE SUMMARY (ground truth).

## REFERENCE SUMMARY (Ground Truth):
{ground_truth}

## GENERATED SUMMARY (To Evaluate):
{llm_summary}

## AUDIT INSTRUCTIONS:

1. **Step 1: Fact Check**: Go sentence by sentence. Does the generated summary contain ANY fact not present in or contradicted by the reference?
   - IF YES: This is a ""Hallucination"". **MAXIMUM SCORE IS 2.**

2. **Step 2: Legal Precision check**: Did the summary miss the specific legal holding or reasoning?
   - IF YES: This is an ""Omission"". **DEDUCT 2 POINTS.**

3. **Step 3: Completeness Check**: Did it miss the dissent or a key concurrent opinion mentioned in the reference?
   - IF YES: **DEDUCT 1 POINT.**

## SCORING RUBRIC (Negative Scoring):
Start at 5.0 and DEDUCT points.
- **5 (Perfect)**: Absolute perfection. No omitted details, no slight inaccuracies. 99% of summaries should NOT get this.
- **4 (Good)**: A minor detail was missed, or a very slight phrasing issue.
- **3 (Mediocre)**: Correct on the main idea, but missed the dissent, or skipped the legal reasoning depth.
- **2 (Failure)**: Contains a factual error (hallucination) OR missed the main holding entirely.
- **1 (Catastrophic)**: Completely wrong case or incoherent.

## RESPOND IN THIS EXACT JSON FORMAT:
```json
{{
    ""factual_accuracy"": <1-5, be harsh>,
    ""completeness"": <1-5, be harsh>,
    ""factual_errors"": [
        {{
            ""error_quote"": ""<exact quote from generated summary>"",
            ""issue"": ""<why is this wrong?>"",
            ""correct_info"": ""<what the reference says>""
        }}
    ],
    ""hedging_detected"": <true/false>,
    ""hedging_examples"": [],
    ""key_omissions"": [""<specific missing item 1>"", ""<specific missing item 2>""],
    ""overall_assessment"": ""<Brutally honest assessment of why points were deducted>""
}}
```

Return ONLY the JSON."
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,1 Ontario v. Quon,google/gemini-2.5-flash-lite,unknown,0.4857,0.8095,0.0000,0.8095,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,1 Ontario v. Quon,x-ai/grok-4.1-fast,unknown,0.3810,0.4762,0.0000,0.8571,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,1 Ontario v. Quon,meta-llama/llama-4-maverick,unknown,0.5190,0.9048,0.0000,0.8095,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,10 California v. Texas,google/gemini-2.5-flash-lite,unknown,0.4818,0.7273,0.0000,0.9091,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,10 California v. Texas,x-ai/grok-4.1-fast,unknown,0.4750,0.7727,0.0000,0.8182,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,10 California v. Texas,meta-llama/llama-4-maverick,unknown,0.4636,0.7727,0.0000,0.7727,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,11 NCAA v. Alston,google/gemini-2.5-flash-lite,unknown,0.5300,0.8000,0.0000,1.0000,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,11 NCAA v. Alston,x-ai/grok-4.1-fast,unknown,0.5067,0.7333,0.0000,1.0000,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,11 NCAA v. Alston,meta-llama/llama-4-maverick,unknown,0.5833,1.0000,0.0000,0.9333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,unknown,0.5000,0.8333,0.0000,0.8333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,unknown,0.4917,0.7500,0.0000,0.9167,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,unknown,0.5000,0.8333,0.0000,0.8333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,unknown,0.4891,0.8696,0.0000,0.7391,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,unknown,0.4913,0.7826,0.0000,0.8696,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,unknown,0.4522,0.8261,0.0000,0.6522,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,unknown,0.4319,0.7778,0.0000,0.6389,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,unknown,0.4250,0.6389,0.0000,0.8056,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,unknown,0.4222,0.8889,0.0000,0.4444,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,15 Kansas v. Glover,google/gemini-2.5-flash-lite,unknown,0.4659,0.6818,0.0000,0.9091,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,15 Kansas v. Glover,x-ai/grok-4.1-fast,unknown,0.4023,0.5000,0.0000,0.9091,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,15 Kansas v. Glover,meta-llama/llama-4-maverick,unknown,0.4818,0.7273,0.0000,0.9091,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,unknown,0.4683,0.7667,0.0000,0.8000,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,unknown,0.4550,0.6333,0.0000,0.9333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,unknown,0.2917,0.8333,0.0000,0.0000,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,17 Gill v. Whitford,google/gemini-2.5-flash-lite,unknown,0.4839,0.8214,0.0000,0.7857,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,17 Gill v. Whitford,x-ai/grok-4.1-fast,unknown,0.4839,0.8214,0.0000,0.7857,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,17 Gill v. Whitford,meta-llama/llama-4-maverick,unknown,0.4750,0.8214,0.0000,0.7500,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,unknown,0.4947,0.7368,0.0000,0.9474,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,unknown,0.4395,0.5789,0.0000,0.9474,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,unknown,0.4079,0.5263,0.0000,0.8947,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,2 Allen v. Michigan,google/gemini-2.5-flash-lite,unknown,0.4625,0.7500,0.0000,0.8000,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,2 Allen v. Michigan,x-ai/grok-4.1-fast,unknown,0.5000,0.7500,0.0000,0.9500,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,2 Allen v. Michigan,meta-llama/llama-4-maverick,unknown,0.2800,0.8000,0.0000,0.0000,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,unknown,0.4471,0.6471,0.0000,0.8824,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,unknown,0.4618,0.6471,0.0000,0.9412,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,unknown,0.4882,0.7647,0.0000,0.8824,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,unknown,0.3625,0.5000,0.0000,0.7500,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,unknown,0.4125,0.5833,0.0000,0.8333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,unknown,0.4125,0.5833,0.0000,0.8333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,unknown,0.4324,0.6471,0.0000,0.8235,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,unknown,0.4265,0.5882,0.0000,0.8824,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,unknown,0.4735,0.7647,0.0000,0.8235,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,unknown,0.5054,0.8571,0.0000,0.8214,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,unknown,0.4679,0.7500,0.0000,0.8214,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,unknown,0.5000,0.8929,0.0000,0.7500,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,unknown,0.4808,0.7692,0.0000,0.8462,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,unknown,0.4731,0.6923,0.0000,0.9231,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,unknown,0.5212,0.8846,0.0000,0.8462,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,7 Torres v. Madrid,google/gemini-2.5-flash-lite,unknown,0.4222,0.6111,0.0000,0.8333,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,7 Torres v. Madrid,x-ai/grok-4.1-fast,unknown,0.3917,0.4444,0.0000,0.9444,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,7 Torres v. Madrid,meta-llama/llama-4-maverick,unknown,0.4361,0.6111,0.0000,0.8889,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,unknown,0.4738,0.8095,0.0000,0.7619,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,unknown,0.4476,0.6667,0.0000,0.8571,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,unknown,0.4786,0.8571,0.0000,0.7143,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,9 Van Buren v. United States,google/gemini-2.5-flash-lite,unknown,0.5180,0.8800,0.0000,0.8400,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,9 Van Buren v. United States,x-ai/grok-4.1-fast,unknown,0.5200,0.8000,0.0000,0.9600,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T14:52:48.248900,2026-02-07T14:52:48.248900_1375938384512,9 Van Buren v. United States,meta-llama/llama-4-maverick,unknown,0.4640,0.8400,0.0000,0.6800,,,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,1 Ontario v. Quon,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8457,0.8095,0.9000,0.8095,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,1 Ontario v. Quon,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7810,0.4762,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,1 Ontario v. Quon,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8390,0.9048,0.8000,0.8095,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,10 California v. Texas,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8018,0.7273,0.8000,0.9091,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,10 California v. Texas,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8750,0.7727,1.0000,0.8182,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,10 California v. Texas,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,11 NCAA v. Alston,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8900,0.8000,0.9000,1.0000,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,11 NCAA v. Alston,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.9067,0.7333,1.0000,1.0000,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,11 NCAA v. Alston,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.9433,1.0000,0.9000,0.9333,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8200,0.8333,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8917,0.7500,1.0000,0.9167,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8200,0.8333,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8091,0.8696,0.8000,0.7391,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8913,0.7826,1.0000,0.8696,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6922,0.8261,0.6000,0.6522,4,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7119,0.7778,0.7000,0.6389,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8250,0.6389,1.0000,0.8056,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6622,0.8889,0.6000,0.4444,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,15 Kansas v. Glover,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7059,0.6818,0.6000,0.9091,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,15 Kansas v. Glover,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8023,0.5000,1.0000,0.9091,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,15 Kansas v. Glover,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8418,0.7273,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7083,0.7667,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8150,0.6333,0.9000,0.9333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,17 Gill v. Whitford,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8439,0.8214,0.9000,0.7857,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,17 Gill v. Whitford,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8839,0.8214,1.0000,0.7857,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,17 Gill v. Whitford,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7550,0.8214,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8147,0.7368,0.8000,0.9474,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8395,0.5789,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6479,0.5263,0.6000,0.8947,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,2 Allen v. Michigan,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7825,0.7500,0.8000,0.8000,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,2 Allen v. Michigan,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8200,0.7500,0.8000,0.9500,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,2 Allen v. Michigan,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7671,0.6471,0.8000,0.8824,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8618,0.6471,1.0000,0.9412,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8882,0.7647,1.0000,0.8824,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6425,0.5000,0.7000,0.7500,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8125,0.5833,1.0000,0.8333,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6925,0.5833,0.7000,0.8333,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7124,0.6471,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8265,0.5882,1.0000,0.8824,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7535,0.7647,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8254,0.8571,0.8000,0.8214,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7879,0.7500,0.8000,0.8214,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7800,0.8929,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7608,0.7692,0.7000,0.8462,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7931,0.6923,0.8000,0.9231,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8812,0.8846,0.9000,0.8462,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,7 Torres v. Madrid,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6622,0.6111,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,7 Torres v. Madrid,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7917,0.4444,1.0000,0.9444,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,7 Torres v. Madrid,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7561,0.6111,0.8000,0.8889,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8338,0.8095,0.9000,0.7619,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8476,0.6667,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8386,0.8571,0.9000,0.7143,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,9 Van Buren v. United States,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7580,0.8800,0.6000,0.8400,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,9 Van Buren v. United States,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8400,0.8000,0.8000,0.9600,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:04:06.925061,2026-02-07T15:04:06.925061_2169801854336,9 Van Buren v. United States,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7840,0.8400,0.8000,0.6800,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,1 Ontario v. Quon,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7257,0.8095,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,1 Ontario v. Quon,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7410,0.4762,0.9000,0.8571,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,1 Ontario v. Quon,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7590,0.9048,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,10 California v. Texas,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7618,0.7273,0.7000,0.9091,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,10 California v. Texas,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8350,0.7727,0.9000,0.8182,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,10 California v. Texas,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,11 NCAA v. Alston,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.8500,0.8000,0.8000,1.0000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,11 NCAA v. Alston,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8667,0.7333,0.9000,1.0000,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,11 NCAA v. Alston,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.9033,1.0000,0.8000,0.9333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7800,0.8333,0.7000,0.8333,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8117,0.7500,0.8000,0.9167,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7800,0.8333,0.7000,0.8333,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7691,0.8696,0.7000,0.7391,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8513,0.7826,0.9000,0.8696,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7322,0.8261,0.7000,0.6522,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6719,0.7778,0.6000,0.6389,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7450,0.6389,0.8000,0.8056,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6222,0.8889,0.5000,0.4444,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,15 Kansas v. Glover,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7459,0.6818,0.7000,0.9091,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,15 Kansas v. Glover,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7623,0.5000,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,15 Kansas v. Glover,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.8418,0.7273,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7083,0.7667,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8150,0.6333,0.9000,0.9333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,17 Gill v. Whitford,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7639,0.8214,0.7000,0.7857,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,17 Gill v. Whitford,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8439,0.8214,0.9000,0.7857,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,17 Gill v. Whitford,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7550,0.8214,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7747,0.7368,0.7000,0.9474,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7995,0.5789,0.9000,0.9474,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6879,0.5263,0.7000,0.8947,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,2 Allen v. Michigan,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7025,0.7500,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,2 Allen v. Michigan,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7400,0.7500,0.6000,0.9500,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,2 Allen v. Michigan,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6871,0.6471,0.6000,0.8824,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8218,0.6471,0.9000,0.9412,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.8082,0.7647,0.8000,0.8824,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6825,0.5000,0.8000,0.7500,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7725,0.5833,0.9000,0.8333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7325,0.5833,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6324,0.6471,0.5000,0.8235,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7865,0.5882,0.9000,0.8824,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6735,0.7647,0.5000,0.8235,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7454,0.8571,0.6000,0.8214,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7479,0.7500,0.7000,0.8214,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7800,0.8929,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7208,0.7692,0.6000,0.8462,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8331,0.6923,0.9000,0.9231,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.8012,0.8846,0.7000,0.8462,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,7 Torres v. Madrid,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6622,0.6111,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,7 Torres v. Madrid,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7517,0.4444,0.9000,0.9444,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,7 Torres v. Madrid,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7961,0.6111,0.9000,0.8889,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7538,0.8095,0.7000,0.7619,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8076,0.6667,0.9000,0.8571,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7986,0.8571,0.8000,0.7143,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,9 Van Buren v. United States,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7580,0.8800,0.6000,0.8400,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,9 Van Buren v. United States,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8000,0.8000,0.7000,0.9600,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.733507,2026-02-07T15:07:21.733507_1367718499136,9 Van Buren v. United States,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7440,0.8400,0.7000,0.6800,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,1 Ontario v. Quon,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8457,0.8095,0.9000,0.8095,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,1 Ontario v. Quon,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7810,0.4762,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,1 Ontario v. Quon,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8390,0.9048,0.8000,0.8095,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,10 California v. Texas,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8018,0.7273,0.8000,0.9091,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,10 California v. Texas,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8750,0.7727,1.0000,0.8182,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,10 California v. Texas,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,11 NCAA v. Alston,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8900,0.8000,0.9000,1.0000,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,11 NCAA v. Alston,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.9067,0.7333,1.0000,1.0000,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,11 NCAA v. Alston,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.9433,1.0000,0.9000,0.9333,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8200,0.8333,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8917,0.7500,1.0000,0.9167,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8200,0.8333,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8091,0.8696,0.8000,0.7391,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8913,0.7826,1.0000,0.8696,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6922,0.8261,0.6000,0.6522,4,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7119,0.7778,0.7000,0.6389,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8250,0.6389,1.0000,0.8056,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6622,0.8889,0.6000,0.4444,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,15 Kansas v. Glover,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7059,0.6818,0.6000,0.9091,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,15 Kansas v. Glover,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8023,0.5000,1.0000,0.9091,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,15 Kansas v. Glover,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8418,0.7273,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7083,0.7667,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8150,0.6333,0.9000,0.9333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,17 Gill v. Whitford,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8439,0.8214,0.9000,0.7857,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,17 Gill v. Whitford,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8839,0.8214,1.0000,0.7857,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,17 Gill v. Whitford,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7550,0.8214,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8147,0.7368,0.8000,0.9474,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8395,0.5789,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6479,0.5263,0.6000,0.8947,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,2 Allen v. Michigan,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7825,0.7500,0.8000,0.8000,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,2 Allen v. Michigan,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8200,0.7500,0.8000,0.9500,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,2 Allen v. Michigan,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7671,0.6471,0.8000,0.8824,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8618,0.6471,1.0000,0.9412,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8882,0.7647,1.0000,0.8824,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6425,0.5000,0.7000,0.7500,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8125,0.5833,1.0000,0.8333,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6925,0.5833,0.7000,0.8333,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7124,0.6471,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8265,0.5882,1.0000,0.8824,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7535,0.7647,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8254,0.8571,0.8000,0.8214,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7879,0.7500,0.8000,0.8214,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7800,0.8929,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7608,0.7692,0.7000,0.8462,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7931,0.6923,0.8000,0.9231,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8812,0.8846,0.9000,0.8462,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,7 Torres v. Madrid,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6622,0.6111,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,7 Torres v. Madrid,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7917,0.4444,1.0000,0.9444,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,7 Torres v. Madrid,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7561,0.6111,0.8000,0.8889,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8338,0.8095,0.9000,0.7619,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8476,0.6667,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8386,0.8571,0.9000,0.7143,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,9 Van Buren v. United States,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7580,0.8800,0.6000,0.8400,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,9 Van Buren v. United States,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8400,0.8000,0.8000,0.9600,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.757820,2026-02-07T15:07:21.757820_1367718418752,9 Van Buren v. United States,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7840,0.8400,0.8000,0.6800,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,1 Ontario v. Quon,google/gemini-2.5-flash-lite,unknown,0.7257,0.8095,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,1 Ontario v. Quon,x-ai/grok-4.1-fast,unknown,0.7810,0.4762,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,1 Ontario v. Quon,meta-llama/llama-4-maverick,unknown,0.7590,0.9048,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,10 California v. Texas,google/gemini-2.5-flash-lite,unknown,0.8018,0.7273,0.8000,0.9091,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,10 California v. Texas,x-ai/grok-4.1-fast,unknown,0.7150,0.7727,0.6000,0.8182,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,10 California v. Texas,meta-llama/llama-4-maverick,unknown,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,11 NCAA v. Alston,google/gemini-2.5-flash-lite,unknown,0.8500,0.8000,0.8000,1.0000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,11 NCAA v. Alston,x-ai/grok-4.1-fast,unknown,0.8267,0.7333,0.8000,1.0000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,11 NCAA v. Alston,meta-llama/llama-4-maverick,unknown,0.8233,1.0000,0.6000,0.9333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,unknown,0.9000,0.8333,1.0000,0.8333,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,unknown,0.7317,0.7500,0.6000,0.9167,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,unknown,0.7800,0.8333,0.7000,0.8333,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,unknown,0.7691,0.8696,0.7000,0.7391,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,unknown,0.8913,0.7826,1.0000,0.8696,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,unknown,0.8522,0.8261,1.0000,0.6522,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,unknown,0.7119,0.7778,0.7000,0.6389,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,unknown,0.6650,0.6389,0.6000,0.8056,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,unknown,0.5822,0.8889,0.4000,0.4444,2,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,15 Kansas v. Glover,google/gemini-2.5-flash-lite,unknown,0.7459,0.6818,0.7000,0.9091,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,15 Kansas v. Glover,x-ai/grok-4.1-fast,unknown,0.8023,0.5000,1.0000,0.9091,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,15 Kansas v. Glover,meta-llama/llama-4-maverick,unknown,0.8018,0.7273,0.8000,0.9091,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,unknown,0.7883,0.7667,0.8000,0.8000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,unknown,0.7750,0.6333,0.8000,0.9333,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,unknown,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,17 Gill v. Whitford,google/gemini-2.5-flash-lite,unknown,0.7239,0.8214,0.6000,0.7857,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,17 Gill v. Whitford,x-ai/grok-4.1-fast,unknown,0.8039,0.8214,0.8000,0.7857,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,17 Gill v. Whitford,meta-llama/llama-4-maverick,unknown,0.7150,0.8214,0.6000,0.7500,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,unknown,0.8947,0.7368,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,unknown,0.8395,0.5789,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,unknown,0.8079,0.5263,1.0000,0.8947,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,2 Allen v. Michigan,google/gemini-2.5-flash-lite,unknown,0.7825,0.7500,0.8000,0.8000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,2 Allen v. Michigan,x-ai/grok-4.1-fast,unknown,0.7400,0.7500,0.6000,0.9500,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,2 Allen v. Michigan,meta-llama/llama-4-maverick,unknown,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,unknown,0.6871,0.6471,0.6000,0.8824,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,unknown,0.7818,0.6471,0.8000,0.9412,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,unknown,0.8082,0.7647,0.8000,0.8824,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,unknown,0.6425,0.5000,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,unknown,0.6525,0.5833,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,unknown,0.7325,0.5833,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,unknown,0.7124,0.6471,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,unknown,0.6665,0.5882,0.6000,0.8824,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,unknown,0.7535,0.7647,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,unknown,0.7454,0.8571,0.6000,0.8214,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,unknown,0.8679,0.7500,1.0000,0.8214,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,unknown,0.7000,0.8929,0.5000,0.7500,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,unknown,0.8808,0.7692,1.0000,0.8462,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,unknown,0.7131,0.6923,0.6000,0.9231,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,unknown,0.7212,0.8846,0.5000,0.8462,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,7 Torres v. Madrid,google/gemini-2.5-flash-lite,unknown,0.5822,0.6111,0.4000,0.8333,2,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,7 Torres v. Madrid,x-ai/grok-4.1-fast,unknown,0.7517,0.4444,0.9000,0.9444,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,7 Torres v. Madrid,meta-llama/llama-4-maverick,unknown,0.6361,0.6111,0.5000,0.8889,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,unknown,0.7938,0.8095,0.8000,0.7619,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,unknown,0.6876,0.6667,0.6000,0.8571,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,unknown,0.7986,0.8571,0.8000,0.7143,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,9 Van Buren v. United States,google/gemini-2.5-flash-lite,unknown,0.7180,0.8800,0.5000,0.8400,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,9 Van Buren v. United States,x-ai/grok-4.1-fast,unknown,0.7200,0.8000,0.5000,0.9600,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:07:21.769148,2026-02-07T15:07:21.769148_1367719418048,9 Van Buren v. United States,meta-llama/llama-4-maverick,unknown,0.7040,0.8400,0.6000,0.6800,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,1 Ontario v. Quon,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7257,0.8095,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,1 Ontario v. Quon,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7410,0.4762,0.9000,0.8571,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,1 Ontario v. Quon,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7590,0.9048,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,10 California v. Texas,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7618,0.7273,0.7000,0.9091,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,10 California v. Texas,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8350,0.7727,0.9000,0.8182,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,10 California v. Texas,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,11 NCAA v. Alston,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.8500,0.8000,0.8000,1.0000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,11 NCAA v. Alston,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8667,0.7333,0.9000,1.0000,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,11 NCAA v. Alston,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.9033,1.0000,0.8000,0.9333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7800,0.8333,0.7000,0.8333,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8117,0.7500,0.8000,0.9167,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7800,0.8333,0.7000,0.8333,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7691,0.8696,0.7000,0.7391,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8513,0.7826,0.9000,0.8696,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7322,0.8261,0.7000,0.6522,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6719,0.7778,0.6000,0.6389,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7450,0.6389,0.8000,0.8056,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6222,0.8889,0.5000,0.4444,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,15 Kansas v. Glover,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7459,0.6818,0.7000,0.9091,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,15 Kansas v. Glover,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7623,0.5000,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,15 Kansas v. Glover,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.8418,0.7273,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7083,0.7667,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8150,0.6333,0.9000,0.9333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,17 Gill v. Whitford,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7639,0.8214,0.7000,0.7857,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,17 Gill v. Whitford,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8439,0.8214,0.9000,0.7857,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,17 Gill v. Whitford,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7550,0.8214,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7747,0.7368,0.7000,0.9474,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7995,0.5789,0.9000,0.9474,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6879,0.5263,0.7000,0.8947,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,2 Allen v. Michigan,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7025,0.7500,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,2 Allen v. Michigan,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7400,0.7500,0.6000,0.9500,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,2 Allen v. Michigan,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6871,0.6471,0.6000,0.8824,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8218,0.6471,0.9000,0.9412,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.8082,0.7647,0.8000,0.8824,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6825,0.5000,0.8000,0.7500,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7725,0.5833,0.9000,0.8333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7325,0.5833,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6324,0.6471,0.5000,0.8235,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7865,0.5882,0.9000,0.8824,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.6735,0.7647,0.5000,0.8235,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7454,0.8571,0.6000,0.8214,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7479,0.7500,0.7000,0.8214,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7800,0.8929,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7208,0.7692,0.6000,0.8462,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8331,0.6923,0.9000,0.9231,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.8012,0.8846,0.7000,0.8462,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,7 Torres v. Madrid,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.6622,0.6111,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,7 Torres v. Madrid,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.7517,0.4444,0.9000,0.9444,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,7 Torres v. Madrid,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7961,0.6111,0.9000,0.8889,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7538,0.8095,0.7000,0.7619,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8076,0.6667,0.9000,0.8571,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7986,0.8571,0.8000,0.7143,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,9 Van Buren v. United States,google/gemini-2.5-flash-lite,anthropic/claude-opus-4.5,0.7580,0.8800,0.6000,0.8400,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,9 Van Buren v. United States,x-ai/grok-4.1-fast,anthropic/claude-opus-4.5,0.8000,0.8000,0.7000,0.9600,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.300587,2026-02-07T15:08:09.300587_1801248340736,9 Van Buren v. United States,meta-llama/llama-4-maverick,anthropic/claude-opus-4.5,0.7440,0.8400,0.7000,0.6800,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,1 Ontario v. Quon,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8457,0.8095,0.9000,0.8095,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,1 Ontario v. Quon,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7810,0.4762,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,1 Ontario v. Quon,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8390,0.9048,0.8000,0.8095,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,10 California v. Texas,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8018,0.7273,0.8000,0.9091,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,10 California v. Texas,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8750,0.7727,1.0000,0.8182,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,10 California v. Texas,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,11 NCAA v. Alston,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8900,0.8000,0.9000,1.0000,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,11 NCAA v. Alston,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.9067,0.7333,1.0000,1.0000,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,11 NCAA v. Alston,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.9433,1.0000,0.9000,0.9333,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8200,0.8333,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8917,0.7500,1.0000,0.9167,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8200,0.8333,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8091,0.8696,0.8000,0.7391,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8913,0.7826,1.0000,0.8696,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6922,0.8261,0.6000,0.6522,4,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7119,0.7778,0.7000,0.6389,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8250,0.6389,1.0000,0.8056,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6622,0.8889,0.6000,0.4444,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,15 Kansas v. Glover,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7059,0.6818,0.6000,0.9091,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,15 Kansas v. Glover,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8023,0.5000,1.0000,0.9091,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,15 Kansas v. Glover,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8418,0.7273,0.9000,0.9091,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7083,0.7667,0.6000,0.8000,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8150,0.6333,0.9000,0.9333,4,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,17 Gill v. Whitford,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8439,0.8214,0.9000,0.7857,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,17 Gill v. Whitford,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8839,0.8214,1.0000,0.7857,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,17 Gill v. Whitford,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7550,0.8214,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8147,0.7368,0.8000,0.9474,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8395,0.5789,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6479,0.5263,0.6000,0.8947,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,2 Allen v. Michigan,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7825,0.7500,0.8000,0.8000,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,2 Allen v. Michigan,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8200,0.7500,0.8000,0.9500,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,2 Allen v. Michigan,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7671,0.6471,0.8000,0.8824,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8618,0.6471,1.0000,0.9412,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8882,0.7647,1.0000,0.8824,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6425,0.5000,0.7000,0.7500,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8125,0.5833,1.0000,0.8333,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.6925,0.5833,0.7000,0.8333,3,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7124,0.6471,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8265,0.5882,1.0000,0.8824,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7535,0.7647,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8254,0.8571,0.8000,0.8214,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7879,0.7500,0.8000,0.8214,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7800,0.8929,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7608,0.7692,0.7000,0.8462,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7931,0.6923,0.8000,0.9231,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8812,0.8846,0.9000,0.8462,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,7 Torres v. Madrid,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.6622,0.6111,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,7 Torres v. Madrid,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.7917,0.4444,1.0000,0.9444,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,7 Torres v. Madrid,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7561,0.6111,0.8000,0.8889,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.8338,0.8095,0.9000,0.7619,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8476,0.6667,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.8386,0.8571,0.9000,0.7143,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,9 Van Buren v. United States,google/gemini-2.5-flash-lite,google/gemini-3-flash-preview,0.7580,0.8800,0.6000,0.8400,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,9 Van Buren v. United States,x-ai/grok-4.1-fast,google/gemini-3-flash-preview,0.8400,0.8000,0.8000,0.9600,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.325105,2026-02-07T15:08:09.325105_1801248326272,9 Van Buren v. United States,meta-llama/llama-4-maverick,google/gemini-3-flash-preview,0.7840,0.8400,0.8000,0.6800,3,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,1 Ontario v. Quon,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7257,0.8095,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,1 Ontario v. Quon,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7810,0.4762,1.0000,0.8571,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,1 Ontario v. Quon,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7590,0.9048,0.6000,0.8095,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,10 California v. Texas,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.8018,0.7273,0.8000,0.9091,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,10 California v. Texas,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7150,0.7727,0.6000,0.8182,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,10 California v. Texas,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7436,0.7727,0.7000,0.7727,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,11 NCAA v. Alston,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.8500,0.8000,0.8000,1.0000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,11 NCAA v. Alston,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.8267,0.7333,0.8000,1.0000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,11 NCAA v. Alston,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.8233,1.0000,0.6000,0.9333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,12 Financial Oversight Board v. Aurelius,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.9000,0.8333,1.0000,0.8333,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,12 Financial Oversight Board v. Aurelius,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7317,0.7500,0.6000,0.9167,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,12 Financial Oversight Board v. Aurelius,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7800,0.8333,0.7000,0.8333,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,13 Espinoza v. Montana Dept of Revenue,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7691,0.8696,0.7000,0.7391,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,13 Espinoza v. Montana Dept of Revenue,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.8913,0.7826,1.0000,0.8696,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,13 Espinoza v. Montana Dept of Revenue,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.8522,0.8261,1.0000,0.6522,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,14 American Legion v. American Humanist Assn,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7119,0.7778,0.7000,0.6389,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,14 American Legion v. American Humanist Assn,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.6650,0.6389,0.6000,0.8056,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,14 American Legion v. American Humanist Assn,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.5822,0.8889,0.4000,0.4444,2,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,15 Kansas v. Glover,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7459,0.6818,0.7000,0.9091,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,15 Kansas v. Glover,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.8023,0.5000,1.0000,0.9091,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,15 Kansas v. Glover,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.8018,0.7273,0.8000,0.9091,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,16 Knick v. Township of Scott,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7883,0.7667,0.8000,0.8000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,16 Knick v. Township of Scott,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7750,0.6333,0.8000,0.9333,5,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,16 Knick v. Township of Scott,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.3717,0.8333,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,17 Gill v. Whitford,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7239,0.8214,0.6000,0.7857,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,17 Gill v. Whitford,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.8039,0.8214,0.8000,0.7857,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,17 Gill v. Whitford,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7150,0.8214,0.6000,0.7500,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,19 Encino Motorcars v. Navarro,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.8947,0.7368,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,19 Encino Motorcars v. Navarro,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.8395,0.5789,1.0000,0.9474,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,19 Encino Motorcars v. Navarro,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.8079,0.5263,1.0000,0.8947,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,2 Allen v. Michigan,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7825,0.7500,0.8000,0.8000,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,2 Allen v. Michigan,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7400,0.7500,0.6000,0.9500,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,2 Allen v. Michigan,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.3600,0.8000,0.2000,0.0000,1,1,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,20 Epic Systems v. Lewis,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.6871,0.6471,0.6000,0.8824,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,20 Epic Systems v. Lewis,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7818,0.6471,0.8000,0.9412,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,20 Epic Systems v. Lewis,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.8082,0.7647,0.8000,0.8824,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,3 Brown v. Board of Education of Topeka,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.6425,0.5000,0.7000,0.7500,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,3 Brown v. Board of Education of Topeka,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.6525,0.5833,0.6000,0.8333,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,3 Brown v. Board of Education of Topeka,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7325,0.5833,0.8000,0.8333,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,"4 Muldrow v. City of St. Louis, Missouri",google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7124,0.6471,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,"4 Muldrow v. City of St. Louis, Missouri",x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.6665,0.5882,0.6000,0.8824,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,"4 Muldrow v. City of St. Louis, Missouri",meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7535,0.7647,0.7000,0.8235,4,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,5 Citizens United v. Federal Election Commission,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7454,0.8571,0.6000,0.8214,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,5 Citizens United v. Federal Election Commission,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.8679,0.7500,1.0000,0.8214,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,5 Citizens United v. Federal Election Commission,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7000,0.8929,0.5000,0.7500,3,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,6 Google LLC v. Oracle America Inc,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.8808,0.7692,1.0000,0.8462,5,5,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,6 Google LLC v. Oracle America Inc,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7131,0.6923,0.6000,0.9231,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,6 Google LLC v. Oracle America Inc,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7212,0.8846,0.5000,0.8462,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,7 Torres v. Madrid,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.5822,0.6111,0.4000,0.8333,2,2,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,7 Torres v. Madrid,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7517,0.4444,0.9000,0.9444,5,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,7 Torres v. Madrid,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.6361,0.6111,0.5000,0.8889,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,8 Fulton v. City of Philadelphia,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7938,0.8095,0.8000,0.7619,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,8 Fulton v. City of Philadelphia,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.6876,0.6667,0.6000,0.8571,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,8 Fulton v. City of Philadelphia,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7986,0.8571,0.8000,0.7143,4,4,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,9 Van Buren v. United States,google/gemini-2.5-flash-lite,minimax/minimax-m2.1,0.7180,0.8800,0.5000,0.8400,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,9 Van Buren v. United States,x-ai/grok-4.1-fast,minimax/minimax-m2.1,0.7200,0.8000,0.5000,0.9600,2,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
2026-02-07T15:08:09.335809,2026-02-07T15:08:09.335809_1801249238912,9 Van Buren v. United States,meta-llama/llama-4-maverick,minimax/minimax-m2.1,0.7040,0.8400,0.6000,0.6800,3,3,EXISTING_SUMMARY,NLI_REFACTOR_RUN
